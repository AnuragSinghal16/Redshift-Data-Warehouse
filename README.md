# Sparkify Song Play Analysis with Redshift

## Project Overview

Sparkify is a startup building a music streaming app. They want to analyze user activity and song play data to better understand customer behavior.
Currently, all their JSON logs of user activity and song metadata reside in AWS S3. Running analytics directly on this raw data is inefficient.

This project builds a cloud-based data warehouse in Amazon Redshift with a star schema optimized for analytical queries on song plays. An ETL pipeline is implemented to load raw data from S3 into staging tables, transform it, and populate the fact and dimension tables.

## Database Schema Design

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset(opens in a new tab). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

Below is an example of a song file:
`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator(opens in a new tab) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.
<img width="1525" height="460" alt="image" src="https://github.com/user-attachments/assets/9170df5d-0c83-407c-8368-23723cf9b8a4" />


The schema follows a star schema pattern for efficient querying.

### Fact Table

`songplays` – records in event data associated with song plays
(songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

### Dimension Tables

`users` – app users
(user_id, first_name, last_name, gender, level)

`songs` – songs in the music database
(song_id, title, artist_id, year, duration)

`artists` – artists in the music database
(artist_id, name, location, latitude, longitude)

`time` – timestamps broken down into units
(start_time, hour, day, week, month, year, weekday)

Staging tables (staging_events, staging_songs) are used to hold raw S3 data before transformation.

## ETL Pipeline

Extract: Load raw JSON data from S3 buckets into staging tables in Redshift.

Event logs: s3://udacity-dend/log_data

Song metadata: s3://udacity-dend/song_data

Transform: Filter, clean, and join staging data.

Load: Insert data into fact and dimension tables following the star schema.

## Project Files

1. `create_tables.py` – drops and creates fact/dimension tables. Run this before ETL.
2. `etl.py` – extracts data from S3, loads into staging tables, transforms and inserts into analytics tables.
3. `sql_queries.py` – contains SQL queries for table creation, dropping, and insertion.
4. `dwh.cfg` – configuration file with Redshift cluster and S3 info. (not uploaded)

## Usage Notes
Use Redshift_Cluster_IaC.py from Data_Engineering_Projects to launch Redshift Cluster. Setup Configurations Setup the dwh.cfg file (File not added in this repository). File format for dwh.cfg

[CLUSTER] HOST='' DB_NAME='' DB_USER='' DB_PASSWORD='' DB_PORT=5439

[IAM_ROLE] ARN=

[S3] LOG_DATA='s3://udacity-dend/log_data' LOG_JSONPATH='s3://udacity-dend/log_json_path.json' SONG_DATA='s3://udacity-dend/song_data'

Then run the test.ipynb
